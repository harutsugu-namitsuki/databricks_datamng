# Databricks notebook source
# MAGIC %md
# MAGIC # 02. ETL Bronzeå±¤ï¼šRDS â†’ ADLS Gen2å–ã‚Šè¾¼ã¿
# MAGIC 
# MAGIC ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€RDS PostgreSQLã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã€Bronzeå±¤ï¼ˆADLS Gen2ï¼‰ã«ä¿å­˜ã—ã¾ã™ã€‚
# MAGIC 
# MAGIC **å‡¦ç†å†…å®¹**:
# MAGIC - RDSã‹ã‚‰JDBCçµŒç”±ã§ãƒ‡ãƒ¼ã‚¿å–å¾—
# MAGIC - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ä¸ï¼ˆ`_load_date`, `_source_system`ï¼‰
# MAGIC - Bronzeå±¤ã«Deltaå½¢å¼ã§ä¿å­˜
# MAGIC - Unity Catalogã«ãƒ†ãƒ¼ãƒ–ãƒ«ç™»éŒ²
# MAGIC 
# MAGIC **å‰ææ¡ä»¶**:
# MAGIC - `00_setup_unity_catalog.py` ãŒå®Ÿè¡Œæ¸ˆã¿ã§ã‚ã‚‹ã“ã¨
# MAGIC - `01_load_northwind_to_rds.py` ã§RDSãƒ‡ãƒ¼ã‚¿ãŒæº–å‚™ã•ã‚Œã¦ã„ã‚‹ã“ã¨

# COMMAND ----------

# MAGIC %md
# MAGIC ## âš ï¸ è¨­å®šå€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„

# COMMAND ----------

# ============================================
# ğŸ‘‡ ã“ã“ã«å®Ÿéš›ã®å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ ğŸ‘‡
# ============================================

# RDSæ¥ç¶šæƒ…å ±ï¼ˆSecretsã‹ã‚‰å–å¾—ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ãŒã€ç°¡ç•¥åŒ–ã®ãŸã‚ç›´æ¥æ›¸ãå ´åˆã¯æ³¨æ„ï¼‰
# â€»æœ¬ç•ªç’°å¢ƒã§ã¯å¿…ãš dbutils.secrets.get() ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
DB_HOST = "premigration-northwind-db.cb0as2s6sr83.ap-southeast-2.rds.amazonaws.com"  # RDSEndpoint
DB_USER = "dbadmin"
DB_PASSWORD = "Yi2345678"
DB_NAME = "northwind"
DB_PORT = 5432

# Unity Catalogè¨­å®š
CATALOG = "northwind_catalog" # ADLSç”¨ã«å¤‰æ›´
BRONZE_SCHEMA = "bronze"

# å‡¦ç†å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«
SOURCE_TABLES = [
    "categories",
    "suppliers", 
    "customers",
    "employees",
    "products",
    "orders",
    "order_details",
    "shippers" # è¿½åŠ 
]

print(f"âœ… è¨­å®šå€¤")
print(f"   DB Host: {DB_HOST}")
print(f"   Catalog: {CATALOG}.{BRONZE_SCHEMA}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## JDBCæ¥ç¶šè¨­å®š

# COMMAND ----------

# JDBC URLæ§‹ç¯‰
jdbc_url = f"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}?sslmode=require"

connection_properties = {
    "user": DB_USER,
    "password": DB_PASSWORD,
    "driver": "org.postgresql.Driver"
}

print(f"âœ… JDBCæ¥ç¶šæº–å‚™å®Œäº†: {DB_HOST}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Catalogã®ä½¿ç”¨è¨­å®š

# COMMAND ----------

# ã‚«ã‚¿ãƒ­ã‚°ãƒ»ã‚¹ã‚­ãƒ¼ãƒã®å­˜åœ¨ç¢ºèªã¨ä½œæˆã¯ 00_setup_unity_catalog.py ã§è¡Œã‚ã‚Œã¦ã„ã‚‹å‰æ

spark.sql(f"USE CATALOG {CATALOG}")
spark.sql(f"USE SCHEMA {BRONZE_SCHEMA}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Bronzeå±¤ã¸ã®ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿é–¢æ•°

# COMMAND ----------

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from datetime import datetime
from pyspark.sql.functions import lit, current_timestamp

def ingest_to_bronze(table_name: str):
    """
    RDSã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€Bronzeå±¤ã«ä¿å­˜ã™ã‚‹
    
    Args:
        table_name: ã‚½ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«å
    """
    print(f"ğŸ“¥ Processing: {table_name}")
    
    # RDSã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    try:
        df = spark.read.jdbc(
            url=jdbc_url,
            table=table_name,
            properties=connection_properties
        )
    except Exception as e:
        print(f"âŒ Error reading from RDS table {table_name}: {e}")
        raise e
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ä¸
    df_with_meta = df \
        .withColumn("_load_date", lit(datetime.now().strftime("%Y-%m-%d"))) \
        .withColumn("_load_timestamp", current_timestamp()) \
        .withColumn("_source_system", lit("rds_northwind"))
    
    # Bronzeå±¤ã«ä¿å­˜ï¼ˆDeltaå½¢å¼ã€Overwriteï¼‰
    # â€»Bronzeã¯Rawãƒ‡ãƒ¼ã‚¿ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚ã‚‹ã„ã¯Appendã¨ã™ã‚‹ã‚±ãƒ¼ã‚¹ãŒå¤šã„ãŒã€
    # å­¦ç¿’ç”¨ã®ãŸã‚ã‚·ãƒ³ãƒ—ãƒ«ã«Overwriteï¼ˆæ´—ã„æ›¿ãˆï¼‰ã¨ã™ã‚‹
    target_table = f"{CATALOG}.{BRONZE_SCHEMA}.{table_name}"
    
    df_with_meta.write \
        .format("delta") \
        .mode("overwrite") \
        .option("mergeSchema", "true") \
        .saveAsTable(target_table)
    
    record_count = df_with_meta.count()
    print(f"âœ… Completed: {target_table} ({record_count} records)")
    
    return record_count

# COMMAND ----------

# MAGIC %md
# MAGIC ## å…¨ãƒ†ãƒ¼ãƒ–ãƒ«å–ã‚Šè¾¼ã¿å®Ÿè¡Œ

# COMMAND ----------

# å–ã‚Šè¾¼ã¿çµæœã‚’è¨˜éŒ²
ingestion_results = []

for table in SOURCE_TABLES:
    try:
        count = ingest_to_bronze(table)
        ingestion_results.append({
            "table": table,
            "status": "success",
            "record_count": count
        })
    except Exception as e:
        print(f"âŒ Error processing {table}: {str(e)}")
        ingestion_results.append({
            "table": table,
            "status": "failed",
            "error": str(e)
        })

# COMMAND ----------

# MAGIC %md
# MAGIC ## å–ã‚Šè¾¼ã¿çµæœã‚µãƒãƒª

# COMMAND ----------

# çµæœã‚’DataFrameã§è¡¨ç¤º
results_df = spark.createDataFrame(ingestion_results)
display(results_df)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Bronzeå±¤ãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèª

# COMMAND ----------

# MAGIC %sql
# MAGIC -- å¤‰æ•°ãŒSQLã‚»ãƒ«ã§ç›´æ¥ä½¿ãˆãªã„ãŸã‚ã€Pythonå¤‰æ•°ã‚’ä¸€æ™‚ãƒ“ãƒ¥ãƒ¼ãªã©ã«æ¸¡ã™ã‹ã€ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã§ç¢ºèª
# MAGIC -- ã“ã“ã§ã¯ SHOW TABLES ã‚’å®Ÿè¡Œ
# MAGIC SHOW TABLES IN northwind_catalog.bronze;

# COMMAND ----------

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç¢ºèªï¼ˆordersï¼‰
# MAGIC %sql
# MAGIC SELECT * FROM northwind_catalog.bronze.orders LIMIT 5;

# COMMAND ----------

# MAGIC %md
# MAGIC ## âœ… å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
# MAGIC 
# MAGIC - [ ] å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ãŒBronzeå±¤ã«å–ã‚Šè¾¼ã¾ã‚ŒãŸ
# MAGIC - [ ] ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆ`_load_date`, `_source_system`ï¼‰ãŒä»˜ä¸ã•ã‚Œã¦ã„ã‚‹
# MAGIC - [ ] Unity Catalog (`northwind_catalog.bronze.*`) ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹
# MAGIC 
# MAGIC æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: `03_etl_silver_transform.py` ã§Silverå±¤ã¸ã®å¤‰æ›ã‚’å®Ÿè¡Œã—ã¾ã™
